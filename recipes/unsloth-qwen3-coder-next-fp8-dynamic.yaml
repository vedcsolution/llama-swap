recipe_version: "1"
name: unsloth/Qwen3-Coder-Next-FP8-Dynamic
description: vLLM serving unsloth/Qwen3-Coder-Next-FP8-Dynamic
runtime: vllm
backend: spark-vllm-docker

model: unsloth/Qwen3-Coder-Next-FP8-Dynamic
container: vllm-node:latest

mods:
  - mods/fix-qwen3-coder-next

defaults:
  port: 8000
  host: 0.0.0.0
  tensor_parallel: 2
  gpu_memory_utilization: 0.81
  max_model_len: 262144

env:
  PYTORCH_CUDA_ALLOC_CONF: "expandable_segments:False"
  # En GB10 / tu setup: NO forzar DeepGEMM para FP8-MoE
  VLLM_USE_DEEP_GEMM: "0"

command: |
  vllm serve unsloth/Qwen3-Coder-Next-FP8-Dynamic \
    --served-model-name unsloth/Qwen3-Coder-Next-FP8-Dynamic \
    --enable-auto-tool-choice \
    --tool-call-parser qwen3_coder \
    --dtype bfloat16 \
    --gpu-memory-utilization {gpu_memory_utilization} \
    --max-model-len {max_model_len} \
    --host {host} \
    --port {port} \
    --kv-cache-dtype fp8 \
    --load-format fastsafetensors \
    --attention-backend flashinfer \
    --enable-prefix-caching \
    --enable-chunked-prefill \
    --max-num-batched-tokens 8192 \
    --max-num-seqs 64 \
    --tensor-parallel-size {tensor_parallel} \
    --distributed-executor-backend ray